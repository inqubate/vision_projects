{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Implement a simple CNN with the following parameters fixed. A Convolutional layer will have 32 neurons (feature maps) and a 5x5 feature detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape(-1, 28, 28, 1)\n",
    "train_images = train_images.astype('float32')\n",
    "train_images /= 255\n",
    "train_labels = to_categorical(train_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_images.reshape(-1, 28, 28, 1)\n",
    "test_images = test_images.astype('float32')\n",
    "test_images /= 255\n",
    "test_labels = to_categorical(test_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1014 10:31:34.819975  7800 deprecation.py:506] From C:\\Users\\Diane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 3.6829 - acc: 0.4517\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.5511 - acc: 0.8351\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.4373 - acc: 0.8709\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.3993 - acc: 0.8812\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.3772 - acc: 0.8874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f18de433c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PYTHONHASHSEED']=\"0\"\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_random_seed(123)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5,5), activation='sigmoid', input_shape=[28, 28, 1]))\n",
    "#model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "model.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images,train_labels,epochs=5,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.3428 - acc: 0.9006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34277879140377043, 0.9006]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 43s 709us/sample - loss: 2.3068 - acc: 0.1073\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 43s 709us/sample - loss: 2.3021 - acc: 0.1098\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 43s 709us/sample - loss: 2.2949 - acc: 0.1249\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 43s 710us/sample - loss: 2.2702 - acc: 0.1722\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 43s 709us/sample - loss: 2.0024 - acc: 0.4766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f1920c0c18>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PYTHONHASHSEED']=\"0\"\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_random_seed(123)\n",
    "model1 = Sequential()\n",
    "model1.add(Conv2D(32, (5,5), activation='sigmoid', input_shape=[28, 28, 1]))\n",
    "model1.add(Conv2D(32, (5,5), activation='sigmoid'))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(128, activation='sigmoid'))\n",
    "model1.add(Dense(10, activation='softmax'))\n",
    "model1.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model1.fit(train_images,train_labels,epochs=5,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 90us/sample - loss: 1.4958 - acc: 0.7058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4958270210266114, 0.7058]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task3 1.How can you improve the models built in Tasks 1 & 2? 2.Using only convolutional layers hyper-parameter optimization (no of layer, no of the nodes, learning rate, etc) help in increasing the accuracy? If yes, implement the changes and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "increase epochs,increase no of the nodes\n",
    "increasing epochs and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 5.6590 - acc: 0.1963\n",
      "Epoch 2/16\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 1.1068 - acc: 0.6788\n",
      "Epoch 3/16\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.6084 - acc: 0.8228\n",
      "Epoch 4/16\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4978 - acc: 0.8563\n",
      "Epoch 5/16\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4448 - acc: 0.8723\n",
      "Epoch 6/16\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4209 - acc: 0.8773\n",
      "Epoch 7/16\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4004 - acc: 0.8838\n",
      "Epoch 8/16\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.3862 - acc: 0.8868\n",
      "Epoch 9/16\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.3757 - acc: 0.8902\n",
      "Epoch 10/16\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.3662 - acc: 0.8918\n",
      "Epoch 11/16\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.3602 - acc: 0.8939\n",
      "Epoch 12/16\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.3541 - acc: 0.8958\n",
      "Epoch 13/16\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.3491 - acc: 0.8971\n",
      "Epoch 14/16\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.3441 - acc: 0.8989\n",
      "Epoch 15/16\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.3415 - acc: 0.8996\n",
      "Epoch 16/16\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.3368 - acc: 0.9009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f192beebe0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PYTHONHASHSEED']=\"0\"\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_random_seed(123)\n",
    "model3 = Sequential()\n",
    "model3.add(Conv2D(32, (5,5), activation='sigmoid', input_shape=[28, 28, 1]))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "model3.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model3.fit(train_images,train_labels,epochs=16,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.3153 - acc: 0.9076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31532849372029303, 0.9076]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 15.7617 - acc: 0.2329\n",
      "Epoch 2/32\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 1.1582 - acc: 0.7026\n",
      "Epoch 3/32\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.6285 - acc: 0.8076\n",
      "Epoch 4/32\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.5221 - acc: 0.8362\n",
      "Epoch 5/32\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.4394 - acc: 0.8629\n",
      "Epoch 6/32\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.4281 - acc: 0.8652\n",
      "Epoch 7/32\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.4072 - acc: 0.8735\n",
      "Epoch 8/32\n",
      "60000/60000 [==============================] - 12s 195us/sample - loss: 0.3894 - acc: 0.8805\n",
      "Epoch 9/32\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.3816 - acc: 0.8834\n",
      "Epoch 10/32\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.3685 - acc: 0.8874\n",
      "Epoch 11/32\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.3643 - acc: 0.8896\n",
      "Epoch 12/32\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.3585 - acc: 0.8913\n",
      "Epoch 13/32\n",
      "60000/60000 [==============================] - 11s 192us/sample - loss: 0.3561 - acc: 0.8922\n",
      "Epoch 14/32\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.3483 - acc: 0.8960\n",
      "Epoch 15/32\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.3516 - acc: 0.8950\n",
      "Epoch 16/32\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.3443 - acc: 0.8960\n",
      "Epoch 17/32\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.3400 - acc: 0.8972\n",
      "Epoch 18/32\n",
      "60000/60000 [==============================] - 11s 192us/sample - loss: 0.3392 - acc: 0.8978\n",
      "Epoch 19/32\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.3374 - acc: 0.8986\n",
      "Epoch 20/32\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.3319 - acc: 0.9011\n",
      "Epoch 21/32\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.3300 - acc: 0.9021\n",
      "Epoch 22/32\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.3293 - acc: 0.9015\n",
      "Epoch 23/32\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.3270 - acc: 0.9025\n",
      "Epoch 24/32\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.3270 - acc: 0.9022\n",
      "Epoch 25/32\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.3237 - acc: 0.9039\n",
      "Epoch 26/32\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.3224 - acc: 0.9043\n",
      "Epoch 27/32\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.3215 - acc: 0.9036\n",
      "Epoch 28/32\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.3196 - acc: 0.9045\n",
      "Epoch 29/32\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.3179 - acc: 0.9060\n",
      "Epoch 30/32\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.3187 - acc: 0.9050\n",
      "Epoch 31/32\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.3163 - acc: 0.9060\n",
      "Epoch 32/32\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.3142 - acc: 0.9069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f1a080d240>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PYTHONHASHSEED']=\"0\"\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_random_seed(123)\n",
    "model4 = Sequential()\n",
    "model4.add(Conv2D(64, (5,5), activation='sigmoid', input_shape=[28, 28, 1]))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(10, activation='softmax'))\n",
    "model4.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model4.fit(train_images,train_labels,epochs=32,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.3138 - acc: 0.9102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3137745406270027, 0.9102]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 55s 913us/sample - loss: 1.0696 - acc: 0.6476\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 55s 912us/sample - loss: 0.4068 - acc: 0.8760\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 55s 919us/sample - loss: 0.2891 - acc: 0.9125\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 55s 917us/sample - loss: 0.2371 - acc: 0.9295\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 55s 918us/sample - loss: 0.2075 - acc: 0.9378\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 55s 918us/sample - loss: 0.1866 - acc: 0.9443\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 55s 916us/sample - loss: 0.1678 - acc: 0.9503\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 55s 923us/sample - loss: 0.1555 - acc: 0.9543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f1a4da6400>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PYTHONHASHSEED']=\"0\"\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_random_seed(123)\n",
    "model5 = Sequential()\n",
    "model5.add(Conv2D(32, (5,5), activation='relu', input_shape=[28, 28, 1]))\n",
    "model5.add(Conv2D(64, (5,5), activation='relu'))\n",
    "model5.add(MaxPool2D(pool_size=(2,2)))\n",
    "model5.add(Flatten())\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(128, activation='relu'))\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(10, activation='softmax'))\n",
    "model5.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model5.fit(train_images,train_labels,epochs=8,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 134us/sample - loss: 0.0728 - acc: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07278091001817956, 0.9764]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5\n",
    "Task 3 Result shows:\n",
    "The accuracy in task 2 is lower than task 1.  one more hidden layer doesn’t improve model's performance, the activation function “sigmoid” cause gradient to vanish.\n",
    "Increasing epochs and neurons, as well as batch size will improve the performance\n",
    "Task 3&4 improve task 1 and Task 2 results. the utilization the activation function relu and increase neurons will improve the model's performance. and the add of pool layer will reduce the training time. the drop out step will improve the generalization ability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
